{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSS GM Lecture 2: Data Mining #\n",
    "<i>Authors: Roshan Lodha & Kevin Chai</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and styling\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is data mining? ###\n",
    "Wikipedia broadly defines data mining as <b>the process of discovering patterns in large data sets</b>. This can involve techniques that span many fields, and has obvious practical impact in things like artificial intelligence. More subtly, appropriate application of data mining can greatly improve tangential fields like healthcare, education, and others. \n",
    "\n",
    "Our goal for today is to is to better define data mining, and go over specific uses in real-world data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0: Data Cleaning ###\n",
    "Before we can search for patterns in our data, we need to ensure that it is clean in order to minimize future errors. Python (specifically pandas) has built in commands that make data cleaning extremely simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('BL-Flickr-Images-Book.csv') #credit: realpython\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataframe is very \"dirty.\" A lot of the information is seemingly unnecessary so we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Edition Statement','Corporate Author','Corporate Contributors',\n",
    "         'Former owner','Engraver','Contributors','Issuance type','Shelfmarks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that the \"Identifier\" column in unique and use that as the pivot over an ordered list of numbers that does not tell us much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Identifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"Date of Publication\" column we can see that the values are not in a consistent format. We can use Regex to extract the useful information. The mechanism and rules for regular expressions are out of the scope of this section but will be covered in the a future lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DoP = df['Date of Publication'].str.extract(r'^(\\d{4})', expand=False)\n",
    "df['Date of Publication'] = pd.to_numeric(DoP)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Clustering ###\n",
    "We'll start by defining a randomly generated dataset using sklearn, and show how data mining can reveal its domains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset creation\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, y_true = make_blobs(n_samples=100, centers=3, cluster_std=1)\n",
    "plt.scatter(X[:, 0], X[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can visually see that there are 3 clusters. While the blobs are easy to seperate in the current stae, extending this to multiple dimensions would render it impossible to distinguish them. That's where clustering algorithms come in; using math not in the scope of this notebook, they can reveal underlying patters in datasets. \n",
    "\n",
    "A very popular and easy to use clusting algorithm is k-means clustering, which computes k centroids in the dataset. \n",
    "\n",
    "Note: In practice, we do not know how many clusters truly define the dataset, so we try a range of values and selected the number that best balances number of clusters with loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "means = KMeans(n_clusters=3)\n",
    "means.fit(X)\n",
    "fitted = means.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the clusters; credit: jakevdp\n",
    "plt.scatter(X[:, 0], X[:, 1], c=fitted, cmap='Accent')\n",
    "centers = means.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='*', c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Regression ###\n",
    "Another data mining technique you have likely heard of is regression. Put simply, regression is a powerful predictive technique that seeks to harness the clumps above into a predictive tool. While there are many different forms of regression (simple, linear, logistic, etc.) we will demo the Random Forest Regressor here.\n",
    "\n",
    "Random Forest works by defining boundaries in a way that seperates clumps effectively while simultanouesly minimizing loss. If you want to read more about how it works and the loss it minimizes read sklearn's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris.data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers above make little sense so we can process them into a dataframe based on sklearn's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame({\n",
    "    'sepal length':iris.data[:,0],\n",
    "    'sepal width':iris.data[:,1],\n",
    "    'petal length':iris.data[:,2],\n",
    "    'petal width':iris.data[:,3],\n",
    "    'species':iris.target\n",
    "})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key difference between clustering and regression is the need for training data in the latter. As the name implies, training data trains the model to predict an outcome given input features. While it is easy to manually split a dataframe, we will use sklearn's test-train-split to make our life easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=data[['sepal length', 'sepal width', 'petal length', 'petal width']]\n",
    "y=data['species']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train our Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we assess how well our model predicted the form of flower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
