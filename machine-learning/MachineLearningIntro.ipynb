{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning and Classification #\n",
    "#### Presented by: Data Science Society ####\n",
    "#### Authors: Roshan Lodha and Varun Murthy ####\n",
    "\n",
    "<i> Credits: This notebook borrows heavily from the Data 100 lecture notebook. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics covered:\n",
    "- exploratory data analysis and feature selection / engineering\n",
    "- loss functions\n",
    "- gradient descent / logistic regression\n",
    "- classification and measures of model integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction ##\n",
    "Let's start by simply loading our packages and data. Lucky for us, sklearn provides many learning datasets that are already cleaned. Since data cleaning is not the focus of this workshop, we'll ignore this portion of the data science life cycle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and loading data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import sklearn.datasets\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "data_raw = sklearn.datasets.load_breast_cancer()\n",
    "breast_cancer_data = pd.DataFrame(data_raw['data'], columns=data_raw['feature_names'])\n",
    "# Target data_dict['target'] = 0 is malignant; 1 is benign\n",
    "breast_cancer_data['malignant'] = 1 - data_raw['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection and Engineering ###\n",
    "\n",
    "Let's begin selecting our features by sifting through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "breast_cancer_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the introduction, the dataset is already cleaned. Thus, we note that there are no null values, and the counts of all the columns are equal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions: ####\n",
    "1) What are some observations you can make about this data? <br>\n",
    "2) What is the granularity of the data (what does each row represent)<br>\n",
    "\n",
    "Planning Ahead:<br>\n",
    "1) What are we trying to figure out from this dataset? <br>\n",
    "\n",
    "Going back to the observations, lets analyze the columns more closely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_data.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are clearly a lot of factors in play, which (based on the column values) seem more or less independent from one another. We can check if this is true using seaborns pairplot function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(breast_cancer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the values have nearly linear relationships! Looking breifly through the pairplot, we can see that specifically row 1, col 3.\n",
    "\n",
    "For the remainder of the notebook, we will ignore this correlation for the sake of simplicity, however this can have significant implications on our results.\n",
    "\n",
    "<b>Before we continue, breifly glance across the bottom row of the pairplot, and think about that that row represents and how we can use it for feature selection.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel('') #add the xlabel here\n",
    "plt.ylabel('malignant')\n",
    "x_vals = breast_cancer_data[''] #choose your features here\n",
    "plt.scatter(x_vals, breast_cancer_data['malignant']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can better approximate the probabilities associated with these features by binning the radii, and calculating the proportion of malignant tumors in each bin.\n",
    "\n",
    "<i>Credit: Data 100 Staff</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binned data logistic simulation\n",
    "radii = np.linspace(5, 30, 50)\n",
    "averages = [np.average(breast_cancer_data[np.abs(breast_cancer_data['worst radius']-r)<2]['malignant']) for r in radii]\n",
    "plt.xlabel('') #add the xlabel here\n",
    "plt.ylabel('malignant')\n",
    "x_vals = breast_cancer_data[''] #enter your feature here\n",
    "plt.scatter(x_vals, breast_cancer_data['malignant']);\n",
    "plt.scatter(radii, averages, color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added intercept column\n",
    "breast_cancer_data['bias'] = 1.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification ###\n",
    "\n",
    "Now that we have selected our features, we can start learning! The first thing we need to do is split the data. This can be easily done using sklearn's train_test_split library.\n",
    "\n",
    "#### Questions: ####\n",
    "1) Hypothesize what a reasonable test-train split size. <br>\n",
    "2) Which column represents the y values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepwork for classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(breast_cancer_data, test_size=?, random_state=100) #enter a test-size below\n",
    "print(\"Training Data Size: \", len(train))\n",
    "print(\"Test Data Size: \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the EDA we did above, lets select some features. Think back onto the pairplot, and which features you think work best, and try them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting features explain the code in this cell, specifically x_train/y_train\n",
    "def features(t):\n",
    "    return t[['feature1', 'feature2', 'feature3']].values.T #replace feature(n) with the desired feature\n",
    "    \n",
    "x_train, y_train = features(train), train['malignant'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to do is fit the data. Again, this can be easily done using sklearn's LogisticRegression library.\n",
    "\n",
    "The math behind this is fairly complex; we will talk about loss minimization and what sklearn is doing behind the scenes while our model is built below. <br>\n",
    "\n",
    "Specifically, we will discuss: <br>\n",
    "1) What do the numbers below mean. <br>\n",
    "2) What are the different loss functions, and why sigmoid loss is used in classification. <br>\n",
    "3) How the loss is being minimized. <br>\n",
    "4) How can we use the model below to classify future inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "breast_cancer_model = LogisticRegression(fit_intercept=False, C=1e9, solver='lbfgs')\n",
    "breast_cancer_model.fit(x_train.T, y_train)\n",
    "breast_cancer_model_features = breast_cancer_model.coef_[0]\n",
    "breast_cancer_model_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(t):\n",
    "    return 1 / (1 + np.e**(-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predictions are in N-dimensional space, where N is the number of features. We can plot 2D cross sections to see the impact of a specific feature below. Take some time to try out all of the features you chose above and make note of any interesting observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train[''], train['malignant'], label = 'original data'); #enter your feature here\n",
    "plt.xlabel('') #enter x-axis label here\n",
    "plt.ylabel('malignant')\n",
    "plt.scatter(radii, averages, color='gold', label = 'binned means');\n",
    "plt.plot(radii, sigma(breast_cancer_model_features[0] + radii * breast_cancer_model_features[1]), color='r', label = 'logistic model');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define some helper functions, predict_prob and classify.\n",
    "\n",
    "<i>Credit: Data 100 Staff </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob(X, betas = breast_cancer_model_features):\n",
    "    return sigma(X.T @ betas)\n",
    "\n",
    "def classify(probabilities, threshold = 0.5):\n",
    "    return np.int64(probabilities > threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the classify function, we see a \"threshold.\" This simply tells the function the raw probability value that marks the cutoff between malignant and benign tumors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting our trained values\n",
    "train_predicted = classify(predict_prob(x_train))\n",
    "train_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can messure how many predictions our model got right or wrong using measures like accuracy, precision, and recall. Shown below is the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trained data accuracy\n",
    "trianing_acc = np.sum(train_predicted == y_train) / len(train_predicted)\n",
    "trianing_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Question:####\n",
    "1) Calculate the train and test mean squared error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing The Model  ###\n",
    "First, we should check how well our model did on test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = features(test), test['malignant'].values\n",
    "test_predicted = classify(predict_prob(x_test))\n",
    "test_acc = np.sum(test_predicted == y_test) / len(test_predicted)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now go back to assess how the threshold affects the accuracy. Try different threshold values to understand the relationship between accuracy and treshold better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try different tresholds here\n",
    "test_predicted = classify(predict_prob(x_test), threshold = ?) #enter threshold here\n",
    "test_acc = np.sum(test_predicted == y_test) / len(test_predicted)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions: ####\n",
    "1) What can't we just pick the treshold that gives us the highest accuracy?<br>\n",
    "\n",
    "<i>Credit: Data 100 Staff</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision and recall\n",
    "def precision_recall(classified, actual):\n",
    "    # It's not necessary to define each of these in both the function for precision\n",
    "    # and recall, but they're here just for the sake of clarity\n",
    "    tp = sum((actual == classified) & (actual == 1))\n",
    "    tn = sum((actual == classified) & (actual == 0))\n",
    "    fp = sum((actual != classified) & (actual == 0))\n",
    "    fn = sum((actual != classified) & (actual == 1))\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    return precision, recall\n",
    "\n",
    "precision, recall = precision_recall(test_predicted, y_test)\n",
    "print('precision = ', precision)\n",
    "print('recall = ', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes our walkthrough of machine learning! Through this notebook, we both explored a dataset, selected and engineered features, and even built a model and assessed how well it functioned. \n",
    "\n",
    "Some notable topics we didn't cover in this workshop: <br>\n",
    "1. K-fold cross validation <br>\n",
    "2. Neural Networks <br>\n",
    "3. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Contest ##\n",
    "Now that you know a little bit about machine learning, try building your own classifier to predict the probability of a basketball team winning given statistics from the National Basketball Association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: This code is copied from Data 100 Course Staff\n",
    "#    It creates the games database to use for the classification problem\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def fetch():\n",
    "    path = 'nba.csv'\n",
    "    if not os.path.exists(path):\n",
    "        url = 'https://stats.nba.com/stats/leaguegamelog/'\n",
    "        params = (\n",
    "            ('Counter', '0'),\n",
    "            ('DateFrom', ''),\n",
    "            ('DateTo', ''),\n",
    "            ('Direction', 'ASC'),\n",
    "            ('LeagueID', '00'),\n",
    "            ('PlayerOrTeam', 'T'),\n",
    "            ('Season', '2017-18'),\n",
    "            ('SeasonType', 'Regular Season'),\n",
    "            ('Sorter', 'DATE'),\n",
    "        )\n",
    "        headers = {\n",
    "            'User-Agent': 'PostmanRuntime/7.4.0'\n",
    "        }\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        data = response.json()['resultSets'][0]\n",
    "        df = pd.DataFrame(data=data['rowSet'], columns=data['headers'])\n",
    "        df.to_csv(path, index=False)\n",
    "        return df\n",
    "    else:\n",
    "        return pd.read_csv(path)\n",
    "    \n",
    "df = fetch()\n",
    "\n",
    "one_team = df.groupby(\"GAME_ID\").first()\n",
    "opponent = df.groupby(\"GAME_ID\").last()\n",
    "games = one_team.merge(opponent, left_index = True, right_index = True, suffixes = [\"\", \"_OPP\"])\n",
    "games[\"FG_PCT_DIFF\"] = games[\"FG_PCT\"] - games[\"FG_PCT_OPP\"]\n",
    "games['WON'] = games['WL'].replace('L', 0).replace('W', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
